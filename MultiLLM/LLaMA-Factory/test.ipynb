{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:670] 2024-09-23 17:25:52,094 >> loading configuration file /home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-09-23 17:25:52,097 >> Model config LlavaConfig {\n",
      "  \"_name_or_path\": \"/home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"hidden_size\": 4096,\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_seq_length\": 1,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"pad_token_id\": 32001,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 1,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"head_dim\": 128,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 4096,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 11008,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"min_length\": 0,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 32,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"prefix\": null,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vision_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 1024,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 336,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 14,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"projection_dim\": 768,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\",\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,100 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,100 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,100 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,100 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,100 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2479] 2024-09-23 17:25:52,151 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|image_processing_base.py:373] 2024-09-23 17:25:52,152 >> loading configuration file /home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:373] 2024-09-23 17:25:52,153 >> loading configuration file /home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt/preprocessor_config.json\n",
      "[INFO|image_processing_base.py:429] 2024-09-23 17:25:52,154 >> Image processor CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,154 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,154 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,154 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,155 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2213] 2024-09-23 17:25:52,155 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2479] 2024-09-23 17:25:52,203 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|processing_utils.py:744] 2024-09-23 17:25:52,271 >> Processor LlavaProcessor:\n",
      "- image_processor: CLIPImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 336,\n",
      "    \"width\": 336\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 336\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: LlamaTokenizerFast(name_or_path='/home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"image_token\": \"<image>\",\n",
      "  \"patch_size\": null,\n",
      "  \"processor_class\": \"LlavaProcessor\",\n",
      "  \"vision_feature_select_strategy\": null\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:670] 2024-09-23 17:25:52,278 >> loading configuration file /home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-09-23 17:25:52,280 >> Model config LlavaConfig {\n",
      "  \"_name_or_path\": \"/home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt\",\n",
      "  \"architectures\": [\n",
      "    \"LlavaForConditionalGeneration\"\n",
      "  ],\n",
      "  \"hidden_size\": 4096,\n",
      "  \"ignore_index\": -100,\n",
      "  \"image_seq_length\": 1,\n",
      "  \"image_token_index\": 32000,\n",
      "  \"model_type\": \"llava\",\n",
      "  \"pad_token_id\": 32001,\n",
      "  \"projector_hidden_act\": \"gelu\",\n",
      "  \"text_config\": {\n",
      "    \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": 1,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"head_dim\": 128,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 4096,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 11008,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 4096,\n",
      "    \"min_length\": 0,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_hidden_layers\": 32,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"prefix\": null,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32064\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.45.0.dev0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vision_config\": {\n",
      "    \"_name_or_path\": \"\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": null,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"begin_suppress_tokens\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"exponential_decay_length_penalty\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_act\": \"quick_gelu\",\n",
      "    \"hidden_size\": 1024,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"image_size\": 336,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"clip_vision_model\",\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_channels\": 3,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": null,\n",
      "    \"patch_size\": 14,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"projection_dim\": 768,\n",
      "    \"pruned_heads\": {},\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"suppress_tokens\": null,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tf_legacy_loss\": false,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vocab_size\": 32000\n",
      "  },\n",
      "  \"vision_feature_layer\": -2,\n",
      "  \"vision_feature_select_strategy\": \"default\",\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "09/23/2024 17:25:52 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3699] 2024-09-23 17:25:52,287 >> loading weights file /home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1621] 2024-09-23 17:25:52,288 >> Instantiating LlavaForConditionalGeneration model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1097] 2024-09-23 17:25:52,289 >> Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 32001\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:1097] 2024-09-23 17:25:52,566 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.53s/it]\n",
      "[INFO|modeling_utils.py:4544] 2024-09-23 17:25:57,723 >> All model checkpoint weights were used when initializing LlavaForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4552] 2024-09-23 17:25:57,724 >> All the weights of LlavaForConditionalGeneration were initialized from the model checkpoint at /home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlavaForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:1050] 2024-09-23 17:25:57,727 >> loading configuration file /home/chuannong1/KJY/MultiLLM/LLaMA-Factory/saves/llava1_5-7b/full/pt/generation_config.json\n",
      "[INFO|configuration_utils.py:1097] 2024-09-23 17:25:57,728 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 32001\n",
      "}\n",
      "\n",
      "09/23/2024 17:25:57 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "09/23/2024 17:25:58 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "09/23/2024 17:25:58 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/llava1_5-7b/lora/sft\n",
      "09/23/2024 17:25:58 - INFO - llamafactory.model.loader - all params: 7,063,427,072\n",
      "Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Loading \"original-fs\" failed\n",
      "Error: Cannot find module 'original-fs'\n",
      "Require stack:\n",
      "- /home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js\n",
      "\u001b[90m    at Module._resolveFilename (node:internal/modules/cjs/loader:1145:15)\u001b[39m\n",
      "\u001b[90m    at Module._load (node:internal/modules/cjs/loader:986:27)\u001b[39m\n",
      "\u001b[90m    at Module.require (node:internal/modules/cjs/loader:1233:19)\u001b[39m\n",
      "\u001b[90m    at require (node:internal/modules/helpers:179:18)\u001b[39m\n",
      "    at i (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:98)\n",
      "    at r.load (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:2:1637)\n",
      "    at h.load (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:13958)\n",
      "    at u (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:9338)\n",
      "    at Object.errorback (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:9457)\n",
      "    at h.triggerErrorback (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:14252)\n",
      "    at /home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:14003\n",
      "    at r.load (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:2:1654)\n",
      "    at h.load (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:13958)\n",
      "    at u (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:9338)\n",
      "    at l._loadModule (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:9466)\n",
      "    at l._resolve (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:4:452)\n",
      "    at l.defineModule (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:3:5561)\n",
      "    at Function.p [as define] (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:4:1741)\n",
      "    at out-build/bootstrap-amd.js (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:4:6445)\n",
      "    at /home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:1:132\n",
      "    at Object.<anonymous> (/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js:4:9653)\n",
      "\u001b[90m    at Module._compile (node:internal/modules/cjs/loader:1358:14)\u001b[39m\n",
      "\u001b[90m    at Module._extensions..js (node:internal/modules/cjs/loader:1416:10)\u001b[39m\n",
      "\u001b[90m    at Module.load (node:internal/modules/cjs/loader:1208:32)\u001b[39m\n",
      "\u001b[90m    at Module._load (node:internal/modules/cjs/loader:1024:12)\u001b[39m\n",
      "\u001b[90m    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:174:12)\u001b[39m\n",
      "\u001b[90m    at node:internal/main/run_main_module:28:49\u001b[39m {\n",
      "  code: \u001b[32m'MODULE_NOT_FOUND'\u001b[39m,\n",
      "  requireStack: [\n",
      "    \u001b[32m'/home/chuannong1/.vscode-server/cli/servers/Stable-fee1edb8d6d72a0ddff41e5f71a671c23ed924b9/server/out/server-cli.js'\u001b[39m\n",
      "  ],\n",
      "  phase: \u001b[32m'loading'\u001b[39m,\n",
      "  moduleId: \u001b[32m'original-fs'\u001b[39m,\n",
      "  neededBy: [ \u001b[32m'fs'\u001b[39m ]\n",
      "}\n",
      "Here are the modules that depend on it:\n",
      "[ \u001b[32m'fs'\u001b[39m ]\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3,4 llamafactory-cli webchat config/webchat.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
